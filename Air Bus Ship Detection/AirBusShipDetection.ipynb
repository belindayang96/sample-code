{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "KtPbLTAZPvRi"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "from multiprocessing.dummy import Pool\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "from skimage.morphology import binary_opening, disk, label\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#I pretty much always choose S&E Library. Those comfy chairs in the corners on at least the main and upper level are fantastic. That being said, there was a post like this about a year ago: \n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cAKs6nPfP1kr"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WMC2buOEP4Pr"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C_arg_4IP4Vq",
        "outputId": "b5123afb-d20b-4c0d-acb4-b7e32c0f0db7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2018.11.29)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.0.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Requirement already satisfied: Unidecode>=0.04.16 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.0.23)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n2AM2BTkYeJ_",
        "outputId": "e17a80bd-6563-4480-8d52-47c56feeb201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#This cell is used to create a folder and download the dataset for kaggle competetion\n",
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "filename = \"/root/.kaggle/kaggle.json\"  # NOTE: This is different from the Medium post!\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)\n",
        "#!kaggle competitions list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download 100%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e4hBF6RSY_lh",
        "outputId": "fd24db5a-90d8-485c-f9a1-5d4b9f85f087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c airbus-ship-detection -p /content/drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_submission_v2.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train_ship_segmentations_v2.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Downloading test_v2.zip to /content/drive\n",
            " 99% 2.11G/2.12G [00:17<00:00, 178MB/s]\n",
            "100% 2.12G/2.12G [00:17<00:00, 131MB/s]\n",
            "Downloading train_v2.zip to /content/drive\n",
            "100% 26.4G/26.4G [04:15<00:00, 145MB/s]\n",
            "100% 26.4G/26.4G [04:15<00:00, 111MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xEfsPkXAiiIj",
        "outputId": "de203993-cc6b-493b-cf5c-754eb3a89bd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "!ls /content/drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model\t\t\t  train_ship_segmentations_v2.csv      train_v2.zip\n",
            "sample_submission_v2.csv  train_ship_segmentations_v2.csv.zip\n",
            "test_v2\t\t\t  train_v2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gqv1mCN0jdXp",
        "outputId": "f63a5aa2-7aa4-4174-8039-cac521f02c4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "#!mkdir /content/drive/test_v2\n",
        "os.chdir('/content/drive/')\n",
        "!ls\n",
        "os.chdir('test_v2/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model\t\t\t  train_ship_segmentations_v2.csv      train_v2.zip\n",
            "sample_submission_v2.csv  train_ship_segmentations_v2.csv.zip\n",
            "test_v2\t\t\t  train_v2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i3xUF2kc8fLR"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kP9nt8uLlMdT"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eEu3eAqop3Dq"
      },
      "cell_type": "code",
      "source": [
        "!unzip -q test_v2.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MpSWp0ZObn5y"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e4pmGk9sTzTA",
        "outputId": "cd442a3a-2789-4771-9567-faf050bee002",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "os.chdir('../')\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model\t\t\t  train_ship_segmentations_v2.csv\n",
            "sample_submission_v2.csv  train_ship_segmentations_v2.csv.zip\n",
            "test_v2\t\t\t  train_v2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S4jGG_KaB_Ye",
        "outputId": "c57675f6-629a-462d-f2ac-9cd6276f4a8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "os.chdir('train_v2/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-4541e752fda6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_v2/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_v2/'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "2YC41WuLTzdI"
      },
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q2uLEEcWb_fD"
      },
      "cell_type": "code",
      "source": [
        "!unzip -q train_v2.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4-sBe8dSejAJ"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "from multiprocessing.dummy import Pool\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "from skimage.morphology import binary_opening, disk, label\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "matplotlib.style.use('fivethirtyeight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NI4JR0i8ezQw"
      },
      "cell_type": "code",
      "source": [
        "os.chdir('../')\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7jS8HMGNeq_R"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "\n",
        "\n",
        "train_dpath = 'train_v2/'\n",
        "test_dpath = 'test_v2/'\n",
        "\n",
        "anno_fpath = 'train_ship_segmentations_v2.csv'\n",
        "bst_model_fpath = 'model/bst_unet.model'\n",
        "\n",
        "sample_submission_fpath = 'sample_submission_v2.csv'\n",
        "submission_fpath = 'submission_1.csv'\n",
        "\n",
        "\n",
        "original_img_size = (768, 768)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_-d_4g3ZfL7R"
      },
      "cell_type": "code",
      "source": [
        "!unzip train_ship_segmentations_v2.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H0khqrWrfFGl"
      },
      "cell_type": "code",
      "source": [
        "annos = pd.read_csv(anno_fpath)\n",
        "annos.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ihvCjkSe8ry"
      },
      "cell_type": "code",
      "source": [
        "annos['EncodedPixels_flag'] = annos['EncodedPixels'].map(lambda v: 1 if isinstance(v, str) else 0)\n",
        "\n",
        "imgs = annos.groupby('ImageId').agg({'EncodedPixels_flag': 'sum'}).reset_index().rename(columns={'EncodedPixels_flag': 'ships'})\n",
        "\n",
        "imgs_w_ships = imgs[imgs['ships'] > 0]\n",
        "imgs_wo_ships = imgs[imgs['ships'] == 0].sample(20000, random_state=69278)\n",
        "\n",
        "selected_imgs = pd.concat((imgs_w_ships, imgs_wo_ships))\n",
        "selected_imgs['has_ship'] = selected_imgs['ships'] > 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l9VyX29He8zA"
      },
      "cell_type": "code",
      "source": [
        "selected_imgs.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s059Ickxe80_"
      },
      "cell_type": "code",
      "source": [
        "#split the database in training and testing\n",
        "train_imgs, val_imgs = train_test_split(selected_imgs, test_size=0.15, stratify=selected_imgs['has_ship'], random_state=69278)\n",
        "\n",
        "train_fnames = train_imgs['ImageId'].values\n",
        "val_fnames = val_imgs['ImageId']\n",
        "\n",
        "train_imgs.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aMU5GvAle83H"
      },
      "cell_type": "code",
      "source": [
        "_, train_fnames = train_test_split(train_fnames, test_size=0.1, random_state=69278)\n",
        "_, val_fnames = train_test_split(val_fnames, test_size=0.1, random_state=69278)\n",
        "#train_imgs.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K1fj1N3me85Q"
      },
      "cell_type": "code",
      "source": [
        "class ImgDataset(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 img_dpath,\n",
        "                 img_fnames,\n",
        "                 img_transform,\n",
        "                 mask_encodings=None,\n",
        "                 mask_size=None,\n",
        "                 mask_transform=None):\n",
        "        self.img_dpath = img_dpath\n",
        "        self.img_fnames = img_fnames\n",
        "        self.img_transform = img_transform\n",
        "\n",
        "        self.mask_encodings = mask_encodings\n",
        "        self.mask_size = mask_size\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # https://github.com/pytorch/vision/issues/9#issuecomment-304224800\n",
        "        seed = np.random.randint(2147483647)\n",
        "\n",
        "        fname = self.img_fnames[i]\n",
        "        fpath = os.path.join(self.img_dpath, fname)\n",
        "        img = Image.open(fpath)\n",
        "        if self.img_transform is not None:\n",
        "            random.seed(seed)\n",
        "            img = self.img_transform(img)\n",
        "\n",
        "        if self.mask_encodings is None:\n",
        "            return img, fname\n",
        "\n",
        "        if self.mask_size is None or self.mask_transform is None:\n",
        "            raise ValueError('If mask_dpath is not None, mask_size and mask_transform must not be None.')\n",
        "\n",
        "        mask = np.zeros(self.mask_size, dtype=np.uint8)\n",
        "        if self.mask_encodings[fname][0] == self.mask_encodings[fname][0]: # NaN doesn't equal to itself\n",
        "            for encoding in self.mask_encodings[fname]:\n",
        "                mask += rle_decode(encoding, self.mask_size)\n",
        "        mask = np.clip(mask, 0, 1)\n",
        "\n",
        "        mask = Image.fromarray(mask)\n",
        "\n",
        "        random.seed(seed)\n",
        "        mask = self.mask_transform(mask)\n",
        "\n",
        "        return img, torch.from_numpy(np.array(mask, dtype=np.int64))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_fnames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eiyKpD8-e87e"
      },
      "cell_type": "code",
      "source": [
        "def rle_decode(mask_rle, shape=(768, 768)):\n",
        "    '''\n",
        "    mask_rle: run-length as string formated (start length)\n",
        "    shape: (height,width) of array to return\n",
        "    Returns numpy array, 1 - mask, 0 - background\n",
        "\n",
        "    '''\n",
        "    s = mask_rle.split()\n",
        "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
        "    ends = starts + lengths\n",
        "    im = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
        "    for lo, hi in zip(starts, ends):\n",
        "        im[lo:hi] = 1\n",
        "    return im.reshape(shape).T\n",
        "\n",
        "def rle_encode(im):\n",
        "    '''\n",
        "    im: numpy array, 1 - mask, 0 - background\n",
        "    Returns run length as string formated\n",
        "    '''\n",
        "    pixels = im.T.flatten()\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    runs[::2] -= 1\n",
        "    return ' '.join(str(x) for x in runs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NGnSPpEwe89t"
      },
      "cell_type": "code",
      "source": [
        "def get_mask_encodings(annos, fnames):\n",
        "    a = annos[annos['ImageId'].isin(fnames)]\n",
        "    return a.groupby('ImageId')['EncodedPixels'].apply(lambda x: x.tolist()).to_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rX2LNmOVe8_7"
      },
      "cell_type": "code",
      "source": [
        "def conv1x1(in_channels, out_channels, groups=1):\n",
        "    return nn.Conv2d(in_channels,\n",
        "                     out_channels,\n",
        "                     kernel_size=1,\n",
        "                     groups=groups,\n",
        "                     stride=1)\n",
        "\n",
        "def conv3x3(in_channels, out_channels, stride=1, padding=1, bias=True, groups=1):\n",
        "    return nn.Conv2d(in_channels,\n",
        "                     out_channels,\n",
        "                     kernel_size=3,\n",
        "                     stride=stride,\n",
        "                     padding=padding,\n",
        "                     bias=bias,\n",
        "                     groups=groups)\n",
        "\n",
        "def upconv2x2(in_channels, out_channels, mode='transpose'):\n",
        "    if mode == 'transpose':\n",
        "        return nn.ConvTranspose2d(in_channels,\n",
        "                                  out_channels,\n",
        "                                  kernel_size=2,\n",
        "                                  stride=2)\n",
        "    else:\n",
        "        return nn.Sequential(\n",
        "            nn.Upsample(mode='bilinear', scale_factor=2),\n",
        "            conv1x1(in_channels, out_channels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v0rqilX-e9CL"
      },
      "cell_type": "code",
      "source": [
        "class DownConv(nn.Module):\n",
        "    \"\"\"\n",
        "    A helper Module that performs 2 convolutions and 1 MaxPool.\n",
        "    A ReLU activation follows each convolution.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, pooling=True):\n",
        "        super(DownConv, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.pooling = pooling\n",
        "\n",
        "        self.conv1 = conv3x3(self.in_channels, self.out_channels)\n",
        "        self.conv2 = conv3x3(self.out_channels, self.out_channels)\n",
        "\n",
        "        if self.pooling:\n",
        "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        before_pool = x\n",
        "        if self.pooling:\n",
        "            x = self.pool(x)\n",
        "        return x, before_pool\n",
        "\n",
        "class UpConv(nn.Module):\n",
        "    \"\"\"\n",
        "    A helper Module that performs 2 convolutions and 1 UpConvolution.\n",
        "    A ReLU activation follows each convolution.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 merge_mode='concat',\n",
        "                 up_mode='transpose'):\n",
        "        super(UpConv, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.merge_mode = merge_mode\n",
        "        self.up_mode = up_mode\n",
        "\n",
        "        self.upconv = upconv2x2(self.in_channels,\n",
        "                                self.out_channels,\n",
        "                                mode=self.up_mode)\n",
        "\n",
        "        if self.merge_mode == 'concat':\n",
        "            self.conv1 = conv3x3(2*self.out_channels,\n",
        "                                 self.out_channels)\n",
        "        else:\n",
        "            # num of input channels to conv2 is same\n",
        "            self.conv1 = conv3x3(self.out_channels, self.out_channels)\n",
        "\n",
        "        self.conv2 = conv3x3(self.out_channels, self.out_channels)\n",
        "\n",
        "    def forward(self, from_down, from_up):\n",
        "        \"\"\" Forward pass\n",
        "        Arguments:\n",
        "            from_down: tensor from the encoder pathway\n",
        "            from_up: upconv'd tensor from the decoder pathway\n",
        "        \"\"\"\n",
        "        from_up = self.upconv(from_up)\n",
        "        if self.merge_mode == 'concat':\n",
        "            x = torch.cat((from_up, from_down), 1)\n",
        "        else:\n",
        "            x = from_up + from_down\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IfFtraK5e9ER"
      },
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    \"\"\" `UNet` class is based on https://arxiv.org/abs/1505.04597\n",
        "    The U-Net is a convolutional encoder-decoder neural network.\n",
        "    Contextual spatial information (from the decoding,\n",
        "    expansive pathway) about an input tensor is merged with\n",
        "    information representing the localization of details\n",
        "    (from the encoding, compressive pathway).\n",
        "    Modifications to the original paper:\n",
        "    (1) padding is used in 3x3 convolutions to prevent loss\n",
        "        of border pixels\n",
        "    (2) merging outputs does not require cropping due to (1)\n",
        "    (3) residual connections can be used by specifying\n",
        "        UNet(merge_mode='add')\n",
        "    (4) if non-parametric upsampling is used in the decoder\n",
        "        pathway (specified by upmode='upsample'), then an\n",
        "        additional 1x1 2d convolution occurs after upsampling\n",
        "        to reduce channel dimensionality by a factor of 2.\n",
        "        This channel halving happens with the convolution in\n",
        "        the tranpose convolution (specified by upmode='transpose')\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, in_channels=3, depth=5,\n",
        "                 start_filts=64, up_mode='transpose',\n",
        "                 merge_mode='concat'):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            in_channels: int, number of channels in the input tensor.\n",
        "                Default is 3 for RGB images.\n",
        "            depth: int, number of MaxPools in the U-Net.\n",
        "            start_filts: int, number of convolutional filters for the\n",
        "                first conv.\n",
        "            up_mode: string, type of upconvolution. Choices: 'transpose'\n",
        "                for transpose convolution or 'upsample' for nearest neighbour\n",
        "                upsampling.\n",
        "        \"\"\"\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        if up_mode in ('transpose', 'upsample'):\n",
        "            self.up_mode = up_mode\n",
        "        else:\n",
        "            raise ValueError(\"\\\"{}\\\" is not a valid mode for \"\n",
        "                             \"upsampling. Only \\\"transpose\\\" and \"\n",
        "                             \"\\\"upsample\\\" are allowed.\".format(up_mode))\n",
        "\n",
        "        if merge_mode in ('concat', 'add'):\n",
        "            self.merge_mode = merge_mode\n",
        "        else:\n",
        "            raise ValueError(\"\\\"{}\\\" is not a valid mode for\"\n",
        "                             \"merging up and down paths. \"\n",
        "                             \"Only \\\"concat\\\" and \"\n",
        "                             \"\\\"add\\\" are allowed.\".format(up_mode))\n",
        "\n",
        "        # NOTE: up_mode 'upsample' is incompatible with merge_mode 'add'\n",
        "        if self.up_mode == 'upsample' and self.merge_mode == 'add':\n",
        "            raise ValueError(\"up_mode \\\"upsample\\\" is incompatible \"\n",
        "                             \"with merge_mode \\\"add\\\" at the moment \"\n",
        "                             \"because it doesn't make sense to use \"\n",
        "                             \"nearest neighbour to reduce \"\n",
        "                             \"depth channels (by half).\")\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.in_channels = in_channels\n",
        "        self.start_filts = start_filts\n",
        "        self.depth = depth\n",
        "\n",
        "        self.down_convs = []\n",
        "        self.up_convs = []\n",
        "\n",
        "        # create the encoder pathway and add to a list\n",
        "        for i in range(depth):\n",
        "            ins = self.in_channels if i == 0 else outs\n",
        "            outs = self.start_filts*(2**i)\n",
        "            pooling = True if i < depth-1 else False\n",
        "\n",
        "            down_conv = DownConv(ins, outs, pooling=pooling)\n",
        "            self.down_convs.append(down_conv)\n",
        "\n",
        "        # create the decoder pathway and add to a list\n",
        "        # - careful! decoding only requires depth-1 blocks\n",
        "        for i in range(depth-1):\n",
        "            ins = outs\n",
        "            outs = ins // 2\n",
        "            up_conv = UpConv(ins, outs, up_mode=up_mode,\n",
        "                merge_mode=merge_mode)\n",
        "            self.up_convs.append(up_conv)\n",
        "\n",
        "        self.conv_final = conv1x1(outs, self.num_classes)\n",
        "\n",
        "        # add the list of modules to current module\n",
        "        self.down_convs = nn.ModuleList(self.down_convs)\n",
        "        self.up_convs = nn.ModuleList(self.up_convs)\n",
        "\n",
        "        self.reset_params()\n",
        "\n",
        "    @staticmethod\n",
        "    def weight_init(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.xavier_normal(m.weight)\n",
        "            nn.init.constant(m.bias, 0)\n",
        "\n",
        "\n",
        "    def reset_params(self):\n",
        "        for i, m in enumerate(self.modules()):\n",
        "            self.weight_init(m)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoder_outs = []\n",
        "\n",
        "        # encoder pathway, save outputs for merging\n",
        "        for i, module in enumerate(self.down_convs):\n",
        "            x, before_pool = module(x)\n",
        "            encoder_outs.append(before_pool)\n",
        "\n",
        "        for i, module in enumerate(self.up_convs):\n",
        "            before_pool = encoder_outs[-(i+2)]\n",
        "            x = module(before_pool, x)\n",
        "\n",
        "        # No softmax is used. This means you need to use\n",
        "        # nn.CrossEntropyLoss is your training script,\n",
        "        # as this module includes a softmax already.\n",
        "        x = self.conv_final(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "831_wLIyf5sr"
      },
      "cell_type": "code",
      "source": [
        "class param:\n",
        "    img_size = (80, 80)\n",
        "    bs = 8\n",
        "    num_workers = 4\n",
        "    lr = 0.0001\n",
        "    epochs = 1\n",
        "    unet_depth = 5\n",
        "    unet_start_filters = 8\n",
        "    log_interval = 70 # less then len(train_dl)\n",
        "\n",
        "channel_means = (0.20166926, 0.28220195, 0.31729624)\n",
        "channel_stds = (0.20769505, 0.18813899, 0.16692209)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X85NJ-Tnf5vT"
      },
      "cell_type": "code",
      "source": [
        "train_tfms = transforms.Compose([transforms.Resize(param.img_size),\n",
        "                                 transforms.RandomRotation(360),\n",
        "                                 transforms.ToTensor(),\n",
        "                                 transforms.Normalize(channel_means, channel_stds)])\n",
        "val_tfms = transforms.Compose([transforms.Resize(param.img_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize(channel_means, channel_stds)])\n",
        "mask_tfms = transforms.Compose([transforms.Resize(param.img_size),\n",
        "                                transforms.RandomRotation(360)])\n",
        "\n",
        "train_dl = DataLoader(ImgDataset(train_dpath,\n",
        "                                 train_fnames,\n",
        "                                 train_tfms,\n",
        "                                 get_mask_encodings(annos, train_fnames),\n",
        "                                 original_img_size,\n",
        "                                 mask_tfms),\n",
        "                      batch_size=param.bs,\n",
        "                      shuffle=True,\n",
        "                      pin_memory=torch.cuda.is_available(),\n",
        "                      num_workers=param.num_workers)\n",
        "val_dl = DataLoader(ImgDataset(train_dpath,\n",
        "                               val_fnames,\n",
        "                               val_tfms,\n",
        "                               get_mask_encodings(annos, val_fnames),\n",
        "                               original_img_size,\n",
        "                               mask_tfms),\n",
        "                    batch_size=param.bs,\n",
        "                    shuffle=False,\n",
        "                    pin_memory=torch.cuda.is_available(),\n",
        "                    num_workers=param.num_workers)\n",
        "\n",
        "model = UNet(2,\n",
        "             depth=param.unet_depth,\n",
        "             start_filts=param.unet_start_filters,\n",
        "             merge_mode='concat').cuda()\n",
        "optim = torch.optim.Adam(model.parameters(), lr=param.lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EgAd2_eDf5x8"
      },
      "cell_type": "code",
      "source": [
        "def get_loss(dl, model):\n",
        "    loss = 0\n",
        "    for X, y in dl:\n",
        "        X, y = Variable(X).cuda(), Variable(y).cuda()\n",
        "        output = model(X)\n",
        "        loss += F.cross_entropy(output, y).iloc[i]\n",
        "    loss = loss / len(dl)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CJvWMUUjf50b"
      },
      "cell_type": "code",
      "source": [
        "iters = []\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "it = 0\n",
        "min_loss = np.inf\n",
        "\n",
        "os.makedirs(os.path.dirname(bst_model_fpath), exist_ok=True)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(param.epochs):\n",
        "    for i, (X, y) in enumerate(train_dl):\n",
        "        X = Variable(X).cuda()  # [N, 1, H, W]\n",
        "        y = Variable(y).cuda()  # [N, H, W] with class indices (0, 1)\n",
        "        output = model(X)  # [N, 2, H, W]\n",
        "        loss = F.cross_entropy(output, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        if (i + 1) % param.log_interval == 0:\n",
        "            it += param.log_interval * param.bs\n",
        "            iters.append(it)\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            model.eval()\n",
        "            val_loss = get_loss(val_dl, model)\n",
        "            model.train()\n",
        "            val_losses.append(val_loss.iloc[i])\n",
        "\n",
        "            if val_loss < min_loss:\n",
        "                torch.save(model.state_dict(), bst_model_fpath)\n",
        "\n",
        "model.eval()\n",
        "val_loss = get_loss(val_dl, model)\n",
        "if val_loss < min_loss:\n",
        "    torch.save(model.state_dict(), bst_model_fpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cGn9LcWPf52I"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cTzymnl6f57w"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3tbXZdCVf5-D"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "svTouR2Rf6Ah"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5OGU6A3Kf6Cg"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q04RyabHf6FI"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fWs8B2HSf6HQ"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lL-7kqpge9Ge"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}